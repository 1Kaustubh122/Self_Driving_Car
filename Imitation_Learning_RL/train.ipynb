{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b238b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fd5a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImitationDataset(Dataset):\n",
    "    def __init__(self, rgb_dir, seg_dir, log_path, transforms=None):\n",
    "        self.rgb_dir = rgb_dir\n",
    "        self.seg_dir = seg_dir\n",
    "        self.log_path = log_path\n",
    "        self.transform = transforms\n",
    "        \n",
    "        with open(log_path, 'r') as f:\n",
    "            self.log_data = json.load(f)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.log_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        rgb_image_path = os.path.join(self.rgb_dir, f\"{index:05d}.png\")\n",
    "        seg_image_path = os.path.join(self.seg_dir, f\"{index:05d}.png\")\n",
    "        \n",
    "        rgb_image = Image.open(rgb_image_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            rgb_tensor = self.transform(rgb_image)\n",
    "        else:\n",
    "            rgb_tensor = transforms.ToTensor()(rgb_image)\n",
    "            \n",
    "        seg_image = np.array(Image.open(seg_image_path))\n",
    "        \n",
    "        lane_mask = np.all(seg_image == [0, 255, 0], axis=2).astype(np.uint8)\n",
    "        obs_mask = np.all(seg_image == [255, 0, 0], axis=2).astype(np.uint8)\n",
    "        \n",
    "        seg_tensor = torch.tensor(np.stack([lane_mask, obs_mask], axis=0), dtype=torch.float32)\n",
    "        \n",
    "        if seg_tensor.shape[1:] != rgb_tensor.shape[1:]:\n",
    "            seg_tensor = F.interpolate(\n",
    "                seg_tensor.unsqueeze(0),\n",
    "                size=rgb_tensor.shape[1:],\n",
    "                mode='nearest'\n",
    "            ).squeeze(0)\n",
    "        \n",
    "        input_tensor = torch.cat([rgb_tensor, seg_tensor], dim=0)\n",
    "                        \n",
    "        control = self.log_data[index]\n",
    "        control_tensor = torch.tensor([\n",
    "            control['steer'],\n",
    "            control['throttle'],\n",
    "            control['brake']\n",
    "        ], dtype=torch.float32)\n",
    "        \n",
    "        return input_tensor, control_tensor\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9cfbb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImitationCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImitationCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(5, 32, kernel_size=5, stride=2, padding=2)  # [B, 32, 180, 320]\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)  # [B, 64, 90, 160]\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1) # [B, 128, 45, 80]\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1) # [B, 256, 23, 40]\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256 * 23 * 40, 512)  # Flattened output from conv4 (keep the 23)\n",
    "        self.fc2 = nn.Linear(512, 3)  # Output: steer, throttle, brake\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c481bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_dir = 'rgb_image'\n",
    "seg_dir = 'seg_image'\n",
    "log_path = 'logs/logs.json'\n",
    "\n",
    "dataset = ImitationDataset(rgb_dir, seg_dir, log_path, None)\n",
    "train_loader = DataLoader(dataset, 32, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ImitationCNN().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs= model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dffbedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "steer_error = 0\n",
    "throttle_error = 0\n",
    "brake_error = 0\n",
    "\n",
    "with torch.no_grad():  # No gradient calculation during evaluation\n",
    "    for inputs, targets in train_loader:  # Using train_loader for demonstration\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        steer_error += F.mse_loss(outputs[:, 0], targets[:, 0], reduction='sum').item()\n",
    "        throttle_error += F.mse_loss(outputs[:, 1], targets[:, 1], reduction='sum').item()\n",
    "        brake_error += F.mse_loss(outputs[:, 2], targets[:, 2], reduction='sum').item()\n",
    "\n",
    "n = len(train_loader.dataset)  # Total number of samples in the dataset\n",
    "\n",
    "print(f\"Steer MSE: {steer_error/n:.4f}, Throttle MSE: {throttle_error/n:.4f}, Brake MSE: {brake_error/n:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a094846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_path = r'logs/logs.json'\n",
    "with open(log_path, 'r') as f:\n",
    "    log_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "17f14b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'img': '09999.png', 'steer': -0.002173948334529996, 'throttle': 0.7269210815429688, 'brake': 0.0, 'imu': {'orientation': {'w': 0.13457519332538814, 'x': 0.0, 'y': 0.0, 'z': -0.9909033844636087}, 'orientation_covariance': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'angular_velocity': {'x': -0.0005980819696560502, 'y': -5.1960101700387895e-05, 'z': -0.005580318626016378}, 'angular_velocity_covariance': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'linear_acceleration': {'x': -0.002654331037774682, 'y': -0.014474133029580116, 'z': 9.810001373291016}, 'linear_acceleration_covariance': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'gnss': {'latitude': 0.0005778344940239322, 'longitude': 0.0005181960624935497}, 'timestamp': 1744467318.1344137}\n"
     ]
    }
   ],
   "source": [
    "x = int(0000 + 9999)\n",
    "print(log_data[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bd45cb8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e71728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = Image.open(\"seg_image/00000.png\")\n",
    "img_np = np.array(img)\n",
    "lane = np.all(img_np == [0, 255, 0], axis=-1).astype(np.uint8)\n",
    "obstacle = np.all(img_np == [255, 0, 0], axis=-1).astype(np.uint8)\n",
    "H, W = lane.shape\n",
    "color_mask = np.zeros((H, W, 3), dtype=np.uint8)\n",
    "color_mask[obstacle == 1] = [255, 0, 0]\n",
    "color_mask[lane == 1] = [0, 255, 0]\n",
    "\n",
    "# seg_mask = np.array(Image.open(seg_image_path))\n",
    "seg_mask = img_np\n",
    "\n",
    "lane_mask = np.all(seg_mask == [0, 255, 0], axis=2).astype(np.uint8)  # H x W\n",
    "obs_mask = np.all(seg_mask == [255, 0, 0], axis=2).astype(np.uint8)   # H x W\n",
    "\n",
    "multi_channel_mask = np.stack([lane_mask, obs_mask], axis=0)  # Shape: (2, H, W)\n",
    "seg_tensor = torch.tensor(multi_channel_mask, dtype=torch.float32)\n",
    "\n",
    "# plt.imshow(seg_tensor)\n",
    "# plt.title(\"Combined Segmentation\")\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1470c773",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_image_path = Image.open(\"seg_image/00000.png\")\n",
    "rgb_image_path = Image.open(\"rgb_image/00000.png\")\n",
    "\n",
    "# rgb_image = cv2.cvtColor(cv2.imread(rgb_image_path), cv2.COLOR_BGR2RGB)\n",
    "# rgb_image = Image.fromarray(rgb_image)\n",
    "\n",
    "\n",
    "# rgb_tensor = transforms.ToTensor()(rgb_image)\n",
    "rgb_image = np.array(rgb_image_path)\n",
    "\n",
    "# Convert RGB image to RGB format (if needed)\n",
    "rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Convert the numpy array to a PIL Image (if needed)\n",
    "rgb_image = Image.fromarray(rgb_image)\n",
    "\n",
    "# Convert RGB image to tensor\n",
    "rgb_tensor = transforms.ToTensor()(rgb_image)\n",
    "\n",
    "seg_image = np.array(seg_image_path)\n",
    "lane_mask = np.all(seg_image == [0, 255, 0], axis=2).astype(np.uint8)\n",
    "obs_mask = np.all(seg_image == [255, 0, 0], axis=2).astype(np.uint8)\n",
    "seg_tensor = torch.tensor(np.stack([lane_mask, obs_mask], axis=0), dtype=torch.float32)\n",
    "\n",
    "if seg_tensor.shape[1:] != rgb_tensor.shape[1:]:\n",
    "    seg_tensor = F.interpolate(\n",
    "        seg_tensor.unsqueeze(0),\n",
    "        size=rgb_tensor.shape[1:],\n",
    "        mode='nearest'\n",
    "    ).squeeze(0)\n",
    "\n",
    "input_tensor = torch.cat([rgb_tensor, seg_tensor], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e07c6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 360, 640])\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c594a3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.4547\n",
      "Epoch [1/50], Val Loss: 0.4452\n",
      "Epoch [2/50], Train Loss: 0.4557\n",
      "Epoch [2/50], Val Loss: 0.4452\n",
      "Epoch [3/50], Train Loss: 0.4552\n",
      "Epoch [3/50], Val Loss: 0.4456\n",
      "Epoch [4/50], Train Loss: 0.4549\n",
      "Epoch [4/50], Val Loss: 0.4462\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "class ImitationDataset(Dataset):\n",
    "    def __init__(self, rgb_dir, seg_dir, log_path):\n",
    "        self.rgb_dir = rgb_dir\n",
    "        self.seg_dir = seg_dir\n",
    "        self.log_path = log_path\n",
    "        with open(log_path, 'r') as f:\n",
    "            self.log_data = json.load(f)\n",
    "        \n",
    "        self.rgb_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.augmentation = transforms.RandomHorizontalFlip(p=0.5)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.log_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rgb_image_path = os.path.join(self.rgb_dir, f\"{index:05d}.png\")\n",
    "        seg_image_path = os.path.join(self.seg_dir, f\"{index:05d}.png\")\n",
    "\n",
    "        rgb_image = Image.open(rgb_image_path)\n",
    "        seg_image = Image.open(seg_image_path)\n",
    "\n",
    "        # Apply augmentation consistently\n",
    "        seed = torch.randint(0, 2**32, (1,)).item()\n",
    "        torch.manual_seed(seed)\n",
    "        rgb_image = self.augmentation(rgb_image)\n",
    "        torch.manual_seed(seed)\n",
    "        seg_image = self.augmentation(seg_image)\n",
    "\n",
    "        rgb_tensor = self.rgb_transform(rgb_image)\n",
    "\n",
    "        seg_image = np.array(seg_image)\n",
    "        lane_mask = np.all(seg_image == [0, 255, 0], axis=2).astype(np.uint8)\n",
    "        obs_mask = np.all(seg_image == [255, 0, 0], axis=2).astype(np.uint8)\n",
    "        seg_tensor = torch.tensor(np.stack([lane_mask, obs_mask], axis=0), dtype=torch.float32)\n",
    "\n",
    "        if seg_tensor.shape[1:] != rgb_tensor.shape[1:]:\n",
    "            seg_tensor = F.interpolate(seg_tensor.unsqueeze(0), size=rgb_tensor.shape[1:], mode='nearest').squeeze(0)\n",
    "\n",
    "        input_tensor = torch.cat([rgb_tensor, seg_tensor], dim=0)\n",
    "\n",
    "        control = self.log_data[index]\n",
    "        control_tensor = torch.tensor([control['steer'], control['throttle'], control['brake']], dtype=torch.float32)\n",
    "\n",
    "        # Adjust steering if flipped\n",
    "        if torch.rand(1).item() < 0.5:  # Matches p=0.5 from RandomHorizontalFlip\n",
    "            control_tensor[0] = -control_tensor[0]\n",
    "\n",
    "        return input_tensor, control_tensor\n",
    "\n",
    "class ImitationCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImitationCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(5, 32, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.fc1 = nn.Linear(256 * 23 * 40, 512)  # Adjust based on input size (e.g., 368x640)\n",
    "        self.fc2 = nn.Linear(512, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        out = self.fc2(x)\n",
    "\n",
    "        steer = torch.tanh(out[:, 0])           # [-1, 1]\n",
    "        throttle = torch.sigmoid(out[:, 1])     # [0, 1]\n",
    "        brake = torch.sigmoid(out[:, 2])        # [0, 1]\n",
    "        return torch.stack([steer, throttle, brake], dim=1)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "rgb_dir = 'rgb_image'\n",
    "seg_dir = 'seg_image'\n",
    "log_path = 'logs/logs.json'\n",
    "\n",
    "dataset = ImitationDataset(rgb_dir, seg_dir, log_path)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model, loss, optimizer, scheduler\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ImitationCNN().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5, verbose=True)\n",
    "\n",
    "# Training loop with validation and early stopping\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Val Loss: {val_loss:.4f}\")\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        trigger_times = 0\n",
    "        # Optionally save model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01785f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla_15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
